# RAG Implementation from Scratch ğŸ§ ğŸ”
This repository demonstrates a **from-scratch implementation** of a simple **Retrieval-Augmented Generation (RAG)**
system using basic Python and NLP concepts â€” without using high-level libraries like `TF-IDFVectorizer`.

## ğŸ“Œ What is RAG?
**Retrieval-Augmented Generation (RAG)** combines:
- ğŸ” **Retrieval**: fetching relevant documents or passages from a knowledge base.
- ğŸ§  **Generation**: using a language model to generate answers based on those documents.
This makes it possible to answer user queries with external knowledge, without needing to retrain the model.

Whatâ€™s Inside?
`RAG_implemention_from_scratch.ipynb`
This Jupyter notebook covers:
- ğŸ“š Document corpus creation  
- ğŸ§¹ Preprocessing (tokenization, cleaning)  
- ğŸ§¾ Manual vector creation (word frequency)  
- ğŸ§® Cosine similarity for retrieval  
- ğŸ§  Simple QA logic based on top document  

## ğŸ› ï¸ How It Works

1. **Corpus** â€“ A set of static sentences used as a knowledge base.  
2. **Query Input** â€“ User provides a question.  
3. **Preprocessing** â€“ Clean and tokenize documents and queries.  
4. **Manual Vectorization** â€“ Count word occurrences in both corpus and query.  
5. **Cosine Similarity** â€“ Compute similarity between query and all documents.  
6. **Answer Generation** â€“ Select the most relevant document and generate a response.  


